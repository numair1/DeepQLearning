{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the environment according to simulation sepcified by Eric Neural Net Paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class FOMS:\n",
    "    def get_reward(self,state,action):\n",
    "        normal_coeff=[(1-action)*(2*(state[0]+state[1])-(state[2]+state[3]))+action*(2*(state[2]+state[3])-(state[0]+state[1])),0.01]\n",
    "        return np.random.normal(loc=normal_coeff[0],scale=normal_coeff[1],size=1)\n",
    "        \n",
    "    def get_next_state(self,current_state,action):\n",
    "        if len(current_state)==0:\n",
    "            return np.random.normal(loc=0,scale=0.25,size=64)\n",
    "        else:\n",
    "            new_state=np.zeros(64)\n",
    "            for i in range(16):\n",
    "                s_t=current_state[i]\n",
    "                #mean and sd for normal distribution of next state vectors \n",
    "                # 4i-3 and 4i-2\n",
    "                norm_coeff_p1=[(1-action)*s_t,0.01*(1-action)+0.25*action]\n",
    "                #coeffs for 4i-1 and 4i\n",
    "                norm_coeff_p2=[(1-action)*s_t,0.01*(1-action)+0.25*action]\n",
    "                \n",
    "                #Populate the state variables by sampling from the normal \n",
    "                #distribution params specified above\n",
    "                new_state[4*(i+1)-1]=np.random.normal(loc=norm_coeff_p2[0],scale=norm_coeff_p2[1],size=1)\n",
    "                new_state[4*(i+1)-2]=np.random.normal(loc=norm_coeff_p2[0],scale=norm_coeff_p2[1],size=1)\n",
    "                new_state[4*(i+1)-4]=np.random.normal(loc=norm_coeff_p1[0],scale=norm_coeff_p1[1],size=1)\n",
    "                new_state[4*(i+1)-3]=np.random.normal(loc=norm_coeff_p1[0],scale=norm_coeff_p1[1],size=1)\n",
    "            return new_state\n",
    "    #Don't need it but putting it just in case\n",
    "    def get_action(self):\n",
    "        return np.random.binomial(1,0.5,1)[0]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our environment made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 0.36740096  0.35198255  0.36365887  0.3491325  -0.22659814 -0.22742933\n",
      " -0.2229184  -0.1999111   0.27895242  0.28630602  0.27195331  0.28656794\n",
      " -0.20551268 -0.22295816 -0.19493619 -0.21282867 -0.01596389  0.00379725\n",
      "  0.00346627 -0.00227742  0.31883244  0.30924713  0.30730164  0.29933591\n",
      " -0.04187928 -0.03635926 -0.04660693 -0.0343696  -0.17134724 -0.19154977\n",
      " -0.18476586 -0.18628574 -0.0088024  -0.00718638 -0.01912129 -0.01154355\n",
      "  0.19411522  0.204038    0.20577921  0.19364711  0.21902144  0.22523132\n",
      "  0.21579502  0.21899785 -0.06987925 -0.09463085 -0.08655176 -0.07885639\n",
      "  0.38430935  0.41143997  0.41176515  0.40118561  0.18818825  0.20588541\n",
      "  0.1895193   0.19164272  0.37608614  0.36771978  0.36678938  0.37832324\n",
      " -0.07764933 -0.0994036  -0.09821317 -0.11214831]\n"
     ]
    }
   ],
   "source": [
    "#Create environment object\n",
    "sim=FOMS()\n",
    "state=sim.get_next_state([],-1)\n",
    "action=sim.get_action()\n",
    "\n",
    "print action\n",
    "\n",
    "new_state=sim.get_next_state(state,action)\n",
    "\n",
    "print new_state\n",
    "#It seems to work!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-Network (Highly Simplified for now, will implement experience reply and other refinements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,64],dtype=tf.float32,name=\"input1\")\n",
    "\n",
    "#Layer 1\n",
    "W = tf.Variable(tf.random_uniform([64,32],0,0.01))\n",
    "Qout1 = tf.matmul(inputs1,W)\n",
    "#Layer 2\n",
    "W1 = tf.Variable(tf.random_uniform([32,2],0,0.01))\n",
    "Qout = tf.matmul(Qout1,W1)\n",
    "\n",
    "predict = tf.argmax(Qout,1,name=\"op_to_restore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f21469bbe712>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-58d4b55ead3c>:1: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model0.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model100.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model200.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model300.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model400.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model500.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model600.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model700.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model800.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model900.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1000.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1100.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1200.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1300.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1400.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1500.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1600.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1700.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1800.ckpt\n",
      "Model saved at: /scratch/nsani/DeepQLearning/model1900.ckpt\n",
      "Percent of succesful episodes: [ 0.04481758]%\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "e = 0.1\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Save the model for every 100th iteration\n",
    "        if i%100==0:\n",
    "            save_path=saver.save(sess,\"/scratch/nsani/DeepQLearning/model\"+str(i)+\".ckpt\")\n",
    "            print \"Model saved at: \"+save_path\n",
    "        #Reset environment and get first new observation\n",
    "        state = FOMS()\n",
    "        s1=state.get_next_state([],1)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 91:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.reshape(s1,(1,64))})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = state.get_action()\n",
    "                \n",
    "            #Get new state and reward from environment\n",
    "            \n",
    "            #s1,r,d,_ = env.step(a[0])\n",
    "            new_state=state.get_next_state(s1,a[0])\n",
    "            r=state.get_reward(s1,a[0])\n",
    "            \n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.reshape(new_state,(1,64))})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.reshape(new_state,(1,64)),nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "#             if d == True:\n",
    "#                 #Reduce chance of random action as we train the model.\n",
    "#                 e = 1./((i/50) + 10)\n",
    "#                 break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained, now calculate marginal mean outcome from the learned Q-function (According to MDP paper specifications: 500 iterations for 90 actions per ietration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92715090476\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  new_saver = tf.train.import_meta_graph('/scratch/nsani/DeepQLearning/model1900.ckpt.meta')\n",
    "  new_saver.restore(sess, tf.train.latest_checkpoint('/scratch/nsani/DeepQLearning'))\n",
    "  graph=tf.get_default_graph()\n",
    "  op_to_restore=graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "  rList=[]\n",
    "  for i in range(500):\n",
    "    sim=FOMS()\n",
    "    s1=sim.get_next_state([],1)\n",
    "    rList.append(0)\n",
    "    for j in range(90):\n",
    "        action=sess.run([op_to_restore],feed_dict={inputs1:np.reshape(s1,(1,64))})\n",
    "        rList[i]+=sim.get_reward(s1,action[0][0])\n",
    "        s1=sim.get_next_state(s1,action[0][0])\n",
    "  print np.mean(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (intel)",
   "language": "python",
   "name": "intel-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
